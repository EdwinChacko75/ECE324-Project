# config.yaml

# Model
model_name: meta-llama/Llama-3.2-3B
# model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B

use_lora: false
weights_path: /home/ubuntu/reasonix/RLHF/models/rlhf_policy/3B_CoT_lora_test #/home/ubuntu/reasonix/CoT/checkpoints/3B_3_5e-05_lora_gsm8k/final_model

# Training / Inference
precision: bfloat16 #bfloat16
batch_size: 64
max_new_tokens: 512 # Effective max length
temperature: 0.7
top_p: 0.95
do_sample: false
early_stopping: true
repetition_penalty: 1.0
num_beams: 3

# Dataset
dataset_name: gsm8k

# Paths
checkpoint_dir: /home/ubuntu/reasonix/inference/checkpoints/
output_file_name: outputs.txt

# Optional: CUDA config (uncomment to use)
cuda_visible_devices: "0,1"
