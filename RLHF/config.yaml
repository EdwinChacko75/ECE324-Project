# config.yaml
dataset:
  train_path: /home/ubuntu/reasonix/RLHF/data/test.jsonl  
  test_path: /home/ubuntu/reasonix/RLHF/data/test.jsonl  
  split: "train"
  use_cache: true

model:
  base_model: meta-llama/Llama-3.2-1B
  lora: true
  weights_path:
  max_length: 512

training:
  reward_model:
    epochs: 0.01
    batch_size: 16
    learning_rate: 2e-5
    logging_steps: 50
    save_steps: 1000000
    metric_steps: 2000
    evaluation_strategy: "steps" 
    save_strategy: "no" 
    fp16: true
    bf16: false
    output_dir: "./models/reward_model"

  rlhf:
    epochs: 3
    batch_size: 4
    learning_rate: 1e-5
    reward_model_dir: "/home/ubuntu/reasonix/RLHF/models/reward_model/Llama-3.2-1B_lora"
    output_dir: "./models/rlhf_policy"
    max_new_tokens: 50
    lora: true