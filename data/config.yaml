# config.yaml
# ===============================
# Model Configuration
# ===============================

# Base model to use for training or inference
# Uncomment one to switch models
# model_name: meta-llama/Llama-3.2-3B-Instruct
# model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B

# ===============================
# Generation / Inference Settings
# ===============================
precision: bfloat16                 # Model precision: float16, bfloat16, or float32
batch_size: 96                      # Number of samples processed per batch
max_new_tokens: 512                # Maximum number of new tokens to generate
temperature: 0.7                    # Sampling temperature (higher = more random)
top_p: 0.95                         # Nucleus/top-p sampling threshold
do_sample: false                   # Whether to sample or use greedy/beam decoding
early_stopping: true               # Stop decoding when all beams finish
repetition_penalty: 1.0            # Penalize repeated tokens (1.0 = no penalty)
num_beams: 3                        # Beam size for beam search (1 = greedy decoding)

# ===============================
# Dataset Configuration
# ===============================
dataset_name: gsm8k                # Dataset to load (e.g., gsm8k)
split: "train"                     # Split to use: train, test, or validation

# ===============================
# Output & Checkpoint Paths
# ===============================
checkpoint_dir: checkpoints/       # Directory for saving checkpoints and logs
output_file: deepseek.jsonl        # Where to save inference output (JSONL format)

# ===============================
# Optional CUDA Configuration
# ===============================
# cuda_visible_devices: "3,4,5"    # Manually specify GPU devices to use (uncomment to enable)
