{
    "model_name": "meta-llama/Llama-3.2-1B-Instruct",
    "use_lora": true,
    "dataset_name": "gsm8k",
    "train_split": "train",
    "eval_split": "test",
    "max_length": 512,
    "shuffle_train": true,
    "shuffle_eval": false,
    "batch_size": 4,
    "eval_batch_size": 4,
    "epochs": 3,
    "learning_rate": "2e-4",
    "weight_decay": 0.01,
    "warmup_steps": 100,
    "lr_scheduler_type": "linear",
    "gradient_accumulation_steps": 1,
    "precision": "float16",
    "logging_steps": 50,
    "save_steps": 200,
    "eval_steps": 100,
    "save_total_limit": 2,
    "evaluation_strategy": "steps",
    "save_strategy": "steps",
    "base_checkpoint_dir": "checkpoints",
    "run_name_prefix": "llama3",
    "gradient_checkpointing": false,
    "fp16_full_eval": false,
    "load_best_model_at_end": false
}